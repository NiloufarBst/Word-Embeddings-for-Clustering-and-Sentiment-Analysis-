{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7707ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                               # linear algebra\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ee42a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer          \n",
    "from nltk.corpus import stopwords                \n",
    "import re                                        \n",
    "from string import punctuation \n",
    "import random                                    \n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d529f7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\adita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f50798c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe_Embedder:\n",
    "    def __init__(self, path):\n",
    "        self.embedding_dict = {}\n",
    "        self.embedding_array = []\n",
    "        self.unk_emb = 0\n",
    "        # Adapted from https://stackoverflow.com/questions/37793118/load-pretrained-GloVe-vectors-in-python\n",
    "        with open(path,'r') as f:\n",
    "            for line in f:\n",
    "                split_line = line.split()\n",
    "                word = split_line[0]\n",
    "                embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "                self.embedding_dict[word] = embedding\n",
    "                self.embedding_array.append(embedding.tolist())\n",
    "        self.embedding_array = np.array(self.embedding_array)\n",
    "        self.embedding_dim = len(self.embedding_array[0])\n",
    "        self.vocab_size = len(self.embedding_array)\n",
    "        self.unk_emb = np.zeros(self.embedding_dim)\n",
    "\n",
    "    # Check if the provided embedding is the unknown embedding.\n",
    "    def is_unk_embed(self, embed):\n",
    "        return np.sum((embed - self.unk_emb) ** 2) < 1e-7\n",
    "    \n",
    "    # Check if the provided string is in the vocabulary.\n",
    "    def token_in_vocab(self, x):\n",
    "        if x in self.embedding_dict and not self.is_unk_embed(self.embedding_dict[x]):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Returns the embedding for a single string and prints a warning if\n",
    "    # the string is unknown to the vocabulary.\n",
    "    # \n",
    "    # If indicate_unk is set to True, the return type will be a tuple of \n",
    "    # (numpy array, bool) with the bool indicating whether the returned \n",
    "    # embedding is the unknown embedding.\n",
    "    #\n",
    "    # If warn_unk is set to False, the method will no longer print warnings\n",
    "    # when used on unknown strings.\n",
    "    def embed_str(self, x, indicate_unk = False, warn_unk = True):\n",
    "        if self.token_in_vocab(x):\n",
    "            if indicate_unk:\n",
    "                return (self.embedding_dict[x], False)\n",
    "            else:\n",
    "                return self.embedding_dict[x]\n",
    "        else:\n",
    "            if warn_unk:\n",
    "                    print(\"Warning: provided word is not part of the vocabulary!\")\n",
    "            if indicate_unk:\n",
    "                return (self.unk_emb, True)\n",
    "            else:\n",
    "                return self.unk_emb\n",
    "\n",
    "    # Returns an array containing the embeddings of each vocabulary token in the provided list.\n",
    "    #\n",
    "    # If include_unk is set to False, the returned list will not include any unknown embeddings.\n",
    "    def embed_list(self, x, include_unk = True):\n",
    "        if include_unk:\n",
    "            embeds = [self.embed_str(word, warn_unk = False).tolist() for word in x]\n",
    "        else:\n",
    "            embeds_with_unk = [self.embed_str(word, indicate_unk=True, warn_unk = False) for word in x]\n",
    "            embeds = [e[0].tolist() for e in embeds_with_unk if not e[1]]\n",
    "            if len(embeds) == 0:\n",
    "                print(\"No known words in input:\" + str(x))\n",
    "                embeds = [self.unk_emb.tolist()]\n",
    "        return np.array(embeds)\n",
    "    \n",
    "    # Finds the vocab words associated with the k nearest embeddings of the provided word. \n",
    "    # Can also accept an embedding vector in place of a string word.\n",
    "    # Return type is a nested list where each entry is a word in the vocab followed by its \n",
    "    # distance from whatever word was provided as an argument.\n",
    "    def find_k_nearest(self, word, k, warn_about_unks = True):\n",
    "        if type(word) == str:\n",
    "            word_embedding, is_unk = self.embed_str(word, indicate_unk = True)\n",
    "        else:\n",
    "            word_embedding = word\n",
    "            is_unk = False\n",
    "        if is_unk and warn_about_unks:\n",
    "            print(\"Warning: provided word is not part of the vocabulary!\")\n",
    "\n",
    "        all_distances = np.sum((self.embedding_array - word_embedding) ** 2, axis = 1) ** 0.5\n",
    "        distance_vocab_index = [[w, round(d, 5)] for w,d,i in zip(self.embedding_dict.keys(), all_distances, range(len(all_distances)))]\n",
    "        distance_vocab_index = sorted(distance_vocab_index, key = lambda x: x[1], reverse = False)\n",
    "        return distance_vocab_index[:k]\n",
    "\n",
    "    def save_to_file(self, path):\n",
    "        with open(path, 'w') as f:\n",
    "            for k in self.embedding_dict.keys():\n",
    "                embedding_str = \" \".join([str(round(s, 5)) for s in self.embedding_dict[k].tolist()])\n",
    "                string = k + \" \" + embedding_str\n",
    "                f.write(string + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2293625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ge = GloVe_Embedder(\"GloVe_Embedder_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2224940",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"IA3-train.csv\")\n",
    "test_data = pd.read_csv(\"IA3-dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10231439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "# Applying the cleaning function to both test and train datasets\n",
    "train_data['text'] = train_data['text'].apply(lambda x: clean_text(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a538f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the text\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda x:word_tokenize(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x:word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a16081dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords (defined in nltk.corpus.stopwords)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words \n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda x : remove_stopwords(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cfb55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing the text entries\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(w) for w in text]  ##Notice the use of text.\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda x : lemmatize_text(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x : lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f1a4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_text(text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(lambda x : concatenate_text(x))\n",
    "test_data['text'] = test_data['text'].apply(lambda x : concatenate_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a080844",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(df, validation_split):\n",
    "    \"\"\"\n",
    "    This function generates the training and validation splits from an input dataframe\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe: pandas dataframe with columns \"text\" and \"target\" (binary)\n",
    "        validation_split: should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the validation split\n",
    "    \n",
    "    Returns:\n",
    "        train_samples: list of strings in the training dataset\n",
    "        val_samples: list of strings in the validation dataset\n",
    "        train_labels: list of labels (0 or 1) in the training dataset\n",
    "        val_labels: list of labels (0 or 1) in the validation dataset      \n",
    "    \"\"\"\n",
    "       \n",
    "    text = df['text'].values.tolist()                         # input text as list\n",
    "    targets = df['sentiment'].values.tolist()                    # targets\n",
    "    \n",
    "#   Preparing the training/validation datasets\n",
    "    \n",
    "    seed = random.randint(1,50)   # random integer in a range (1, 50)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(text)\n",
    "    rng = np.random.RandomState(seed)\n",
    "    rng.shuffle(targets)\n",
    "\n",
    "    num_validation_samples = int(validation_split * len(text))\n",
    "\n",
    "    train_samples = text[:-num_validation_samples]\n",
    "    val_samples = text[-num_validation_samples:]\n",
    "    train_labels = targets[:-num_validation_samples]\n",
    "    val_labels = targets[-num_validation_samples:]\n",
    "    \n",
    "    print(f\"Total size of the dataset: {df.shape[0]}.\")\n",
    "    print(f\"Training dataset: {len(train_samples)}.\")\n",
    "    print(f\"Validation dataset: {len(val_samples)}.\")\n",
    "    \n",
    "    return train_samples, val_samples, train_labels, val_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2421108c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the dataset: 9000.\n",
      "Training dataset: 8100.\n",
      "Validation dataset: 900.\n"
     ]
    }
   ],
   "source": [
    "train_samples, val_samples, train_labels, val_labels = train_val_split(train_data, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84acc582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the dataset: 2500.\n",
      "Training dataset: 2250.\n",
      "Validation dataset: 250.\n"
     ]
    }
   ],
   "source": [
    "train_samples, val_samples, train_labels, val_labels = train_val_split(test_data, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5e1f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedded_matrix = []\n",
    "for i in range(0,len(train_data['text'])):\n",
    "    word = train_data['text'][i].split()\n",
    "    train_embedded_matrix.append(ge.embed_list(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ba1f9689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_embedded_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8b97dd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embedded_matrix = []\n",
    "for i in range(0,len(test_data['text'])):\n",
    "    word = test_data['text'][i].split()\n",
    "    test_embedded_matrix.append(ge.embed_list(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "100aef92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "print(len(test_embedded_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1eb6b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a32cd107",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec_average = []\n",
    "for i in range(len(train_embedded_matrix)):\n",
    "    vec_array = np.array(train_embedded_matrix[i])\n",
    "    average = vec_array.mean(axis=0)\n",
    "    word_vec_average.append(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5201b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word_vec_average = []\n",
    "for i in range(len(test_embedded_matrix)):\n",
    "    vec_array = np.array(train_embedded_matrix[i])\n",
    "    average = vec_array.mean(axis=0)\n",
    "    test_word_vec_average.append(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52f953ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e166cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data['text']\n",
    "y_train = train_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5fe1732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:  {'precision': 0.8796007485963818, 'recall': 0.7692307692307693, 'f1-score': 0.820721769499418, 'support': 1833}\n",
      "negative:  {'precision': 0.9428146545896985, 'recall': 0.9730710199525604, 'f1-score': 0.9577039274924471, 'support': 7167}\n"
     ]
    }
   ],
   "source": [
    "linear_svm = svm.SVC(kernel ='linear', C = 10**1).fit(word_vec_average, y_train)\n",
    "prediction1 = linear_svm.predict(word_vec_average)\n",
    "report = classification_report(train_data['sentiment'], prediction1, output_dict=True)\n",
    "print('positive: ', report['1'])\n",
    "print('negative: ', report['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16104307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:  {'precision': 0.18468468468468469, 'recall': 0.15648854961832062, 'f1-score': 0.1694214876033058, 'support': 524}\n",
      "negative:  {'precision': 0.7850194552529183, 'recall': 0.8168016194331984, 'f1-score': 0.8005952380952381, 'support': 1976}\n"
     ]
    }
   ],
   "source": [
    "linear_svm = svm.SVC(kernel ='linear', C = 10**1)\n",
    "training_model = linear_svm.fit(word_vec_average, y_train)\n",
    "prediction_validation = linear_svm.predict(test_word_vec_average)\n",
    "validation_report = classification_report(test_data['sentiment'], prediction_validation, output_dict=True)\n",
    "print('positive: ', validation_report['1'])\n",
    "print('negative: ', validation_report['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfa3f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:  {'precision': 0.18888888888888888, 'recall': 0.16221374045801526, 'f1-score': 0.17453798767967144, 'support': 524}\n",
      "negative:  {'precision': 0.7858536585365854, 'recall': 0.8152834008097166, 'f1-score': 0.8002980625931446, 'support': 1976}\n"
     ]
    }
   ],
   "source": [
    "quadratic_svm = svm.SVC(kernel ='poly', degree=2, C = 10**1, coef0 = 40)\n",
    "training_model = quadratic_svm.fit(word_vec_average, y_train)\n",
    "prediction_validation = quadratic_svm.predict(test_word_vec_average)\n",
    "validation_report = classification_report(test_data['sentiment'], prediction_validation, output_dict=True)\n",
    "print('positive: ', validation_report['1'])\n",
    "print('negative: ', validation_report['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d8d9095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6724\n",
      "positive:  {'precision': 0.19206680584551147, 'recall': 0.17557251908396945, 'f1-score': 0.18344965104685942, 'support': 524}\n",
      "negative:  {'precision': 0.7862444334487877, 'recall': 0.8041497975708503, 'f1-score': 0.7950963222416813, 'support': 1976}\n"
     ]
    }
   ],
   "source": [
    "rbf_svm = svm.SVC(kernel ='rbf', C = 11, gamma=0.2)\n",
    "training_model = rbf_svm.fit(word_vec_average, y_train)\n",
    "prediction_validation = rbf_svm.predict(test_word_vec_average)\n",
    "validation_report = classification_report(test_data['sentiment'], prediction_validation, output_dict=True)\n",
    "print('Accuracy: ', validation_report['accuracy'])\n",
    "print('positive: ', validation_report['1'])\n",
    "print('negative: ', validation_report['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570ab97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
